{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_NAME='data/pdf/sample-pdf.pdf'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#%pip install logging tiktoken azure-ai-documentintelligence azure-ai-documentanalytics azure-core azure-identity PyPDF2 python-dotenv\n",
    "\n",
    "import os\n",
    "import re\n",
    "import tiktoken\n",
    "import logging\n",
    "from io import BytesIO\n",
    "from dotenv import load_dotenv\n",
    "from PyPDF2 import PdfReader\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.documentintelligence import DocumentIntelligenceClient\n",
    "from azure.ai.documentintelligence.models import ContentFormat, AnalyzeResult\n",
    "\n",
    "load_dotenv()\n",
    "encoding = tiktoken.get_encoding(\"cl100k_base\")\n",
    "logger = logging.getLogger()\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "    handlers=[\n",
    "        logging.FileHandler(\"debug.log\"),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "def handle_pdf_locally(uploaded_file, clean=False):\n",
    "    logger.info(\"Processing document locally\")\n",
    "    try:\n",
    "        pdf_reader = PdfReader(uploaded_file)\n",
    "        texts = [page.extract_text() for page in pdf_reader.pages]\n",
    "        if clean: \n",
    "            return clean_text('\\n'.join(texts))\n",
    "        else:\n",
    "            return '\\n'.join(texts)\n",
    "        \n",
    "    except Exception as e:\n",
    "        return logger.error(e, \"Error processing document:\")\n",
    "\n",
    "def handle_pdf_remotely(uploaded_file, clean=False):   \n",
    "    logger.info(\"Processing PDF document remotely\")\n",
    "    try:\n",
    "        doc_intelligence_endpoint = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ENDPOINT\")\n",
    "        doc_intelligence_key = os.getenv(\"AZURE_DOCUMENT_INTELLIGENCE_ADMIN_KEY\")\n",
    "       \n",
    "        document_intelligence_client = DocumentIntelligenceClient(\n",
    "            endpoint=doc_intelligence_endpoint, credential=AzureKeyCredential(doc_intelligence_key)\n",
    "        )    \n",
    "        poller = document_intelligence_client.begin_analyze_document(\n",
    "            \"prebuilt-layout\", \n",
    "            analyze_request=uploaded_file,\n",
    "            content_type=\"application/octet-stream\", \n",
    "            output_content_format=ContentFormat.MARKDOWN)       \n",
    "        result: AnalyzeResult = poller.result()\n",
    "        if clean: \n",
    "            return clean_text(result.content)\n",
    "        else:\n",
    "            return result.content\n",
    "    except Exception as e:\n",
    "        return logger.error(e, \"Error processing PDF document remotely:\")\n",
    "\n",
    "def read_file_bin(file_name: str) -> BytesIO:\n",
    "    \"\"\"\n",
    "    Reads a file and returns its content.\n",
    "    \n",
    "    Parameters:\n",
    "    file_name (str): The name of the file to read.\n",
    "\n",
    "    Returns:\n",
    "    BytesIO: The content of the file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Reading file {file_name}\")\n",
    "    try:\n",
    "        with open(file_name, \"rb\") as file:\n",
    "            return BytesIO(file.read())\n",
    "    except FileNotFoundError:\n",
    "        return BytesIO(b\"The file does not exist.\")\n",
    "    \n",
    "def save_file(file_name: str, data: str) -> None:\n",
    "    \"\"\"\n",
    "    Writes data to a file.\n",
    "    \n",
    "    Parameters:\n",
    "    file_name (str): The name of the file to write to.\n",
    "    data (str): The data to write to the file.\n",
    "    \"\"\"\n",
    "    logger.info(f\"Saving file {file_name}\")\n",
    "    with open(file_name, \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(data)\n",
    "        \n",
    "def num_tokens_from_string(string: str) -> int:\n",
    "    return len(encoding.encode(string))\n",
    "\n",
    "def print_chunks_page_content(page_content):\n",
    "    print(f\"Number of chunks: {len(page_content)}\")\n",
    "    for i, chunk in enumerate(page_content):\n",
    "        print(f\"Chunk {i + 1} character count: {len(chunk.page_content)} token number: {num_tokens_from_string(chunk.page_content)}\" )\n",
    "        print(chunk.page_content)\n",
    "        print()\n",
    "\n",
    "def clean_text(text, remove_comments=False, put_html_tables_on_new_line=True):\n",
    "    logger.info(f\"Cleaning text\")\n",
    "    text = re.sub('(?<=<table>)(.*?)(?=</table>)', lambda m: m.group(0).replace('\\n', ' '), text, flags=re.DOTALL)\n",
    "    patterns = {\n",
    "        '\\n+': '\\n',\n",
    "        ' +': ' ',\n",
    "        r'\\s<': '<',\n",
    "        r'>\\s': '>',\n",
    "        r'\\s\\.': '.',\n",
    "        r'\\s,': ',',\n",
    "        r'\\s!': '!',\n",
    "        r'\\s\\?': '?',\n",
    "        r'\\s:': ':',\n",
    "        r'\\s;': ';',\n",
    "        r'\\s\\)': ')',\n",
    "        r'\\(\\s': '(',\n",
    "        r'\\[\\s': '[',\n",
    "        r'\\s\\]': ']',\n",
    "        r'\\s\\}': '}',\n",
    "        r'\\}\\s': '}',\n",
    "    }\n",
    "    for pattern, replacement in patterns.items():\n",
    "        text = re.sub(pattern, replacement, text)\n",
    "    if put_html_tables_on_new_line:\n",
    "        text = text.replace('<table>', '\\n<table>')\n",
    "    if remove_comments:\n",
    "        text = re.sub(r'<!--(.*?)-->', '', text, flags=re.DOTALL)\n",
    "    return text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Proces document in DI and by PyPDF2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "file=read_file_bin(FILE_NAME)\n",
    "md_file=handle_pdf_remotely(file)\n",
    "txt_file=handle_pdf_locally(file)\n",
    "\n",
    "save_file(FILE_NAME.replace(\".pdf\",\".md\"),md_file)\n",
    "save_file(FILE_NAME.replace(\".pdf\",\".txt\"),txt_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by headers "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from langchain.text_splitter import MarkdownHeaderTextSplitter\n",
    "headers_to_split_on = [\n",
    "    (\"#\", \"Header 1\"),\n",
    "    (\"##\", \"Header 2\"),\n",
    "    (\"###\", \"Header 3\"),\n",
    "    (\"####\", \"Header 4\"),\n",
    "    (\"#####\", \"Header 5\"),\n",
    "    (\"######\", \"Header 6\"),  \n",
    "    (\"#######\", \"Header 7\"), \n",
    "    (\"########\", \"Header 8\")\n",
    "]\n",
    "md_text_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "\n",
    "md_header_splits = md_text_splitter.split_text(md_file)\n",
    "\n",
    "print(\"Length of splits: \" + str(len(md_header_splits)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chunks_page_content(md_header_splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split by characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 750\n",
    "chunk_overlap = 50\n",
    "rct_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "\n",
    "splits = rct_text_splitter.split_documents(md_header_splits)\n",
    "splits\n",
    "\n",
    "print_chunks_page_content(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clean output before split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_file_clean = clean_text(md_file,remove_comments=True)\n",
    "print(md_file_clean)\n",
    "save_file(FILE_NAME.replace(\".pdf\",\"-clean.md\"),md_file_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md_header_splits_clean = md_text_splitter.split_text(md_file_clean)\n",
    "print(\"Length of splits: \" + str(len(md_header_splits_clean)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_chunks_page_content(md_header_splits_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "chunk_size = 750\n",
    "chunk_overlap = 50\n",
    "rct_text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=chunk_size, chunk_overlap=chunk_overlap\n",
    ")\n",
    "\n",
    "splits = rct_text_splitter.split_documents(md_header_splits_clean)\n",
    "splits\n",
    "\n",
    "print_chunks_page_content(splits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unstructred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unstructured.partition.md import partition_md\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "NEW_AFTER_N_CHARS = 1500\n",
    "MAX_CHARACTERS = 1500\n",
    "COMBINE_UNDER_N_CHARS = 500\n",
    "\n",
    "elements = partition_md(text=md_file_clean)\n",
    "print (f\"Number of elements: {len(elements)}\")\n",
    "\n",
    "chunks = chunk_by_title(elements, multipage_sections=True, max_characters=MAX_CHARACTERS, new_after_n_chars=NEW_AFTER_N_CHARS, combine_text_under_n_chars=COMBINE_UNDER_N_CHARS)  \n",
    "out_text=''\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "for i, chunk in enumerate(chunks):           \n",
    "    if chunk.category == 'Table':\n",
    "        chunk_text = chunk.metadata.text_as_html\n",
    "    else:\n",
    "        chunk_text = chunk.text\n",
    "    print(f'Chunk {i} ({chunk.category}): Chunk len ({len(chunk_text)}) Chunk token ({num_tokens_from_string(chunk_text)}) \\n{chunk_text}') \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Semchunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import semchunk\n",
    "chunker = semchunk.chunkerify('gpt-4', chunk_size) \n",
    "data= chunker(md_file_clean) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_chunks(list_chunks):\n",
    "    print(f\"Number of chunks: {len(list_chunks)}\")\n",
    "    for i, chunk in enumerate(list_chunks):\n",
    "        print(f\"\\nChunk {i + 1} character count: {len(chunk)} token number: {num_tokens_from_string(chunk)}\" )\n",
    "        print(chunk)\n",
    "print_chunks (data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
