{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fixed-size Splitting\n",
    "\n",
    "Character splitting is the most fundamental method of breaking down your text. It involves dividing the text into chunks of a specified number of characters, regardless of the content or structure.\n",
    "\n",
    "While this method is generally not recommended for practical applications, it serves as an excellent starting point for understanding the basics of text segmentation.\n",
    "\n",
    "#### Key Concepts:\n",
    "\n",
    "- **Chunk Size**: This is the number of characters you want each chunk to contain. It can be any number, such as 50, 100, or even 100,000 characters.\n",
    "\n",
    "- **Chunk Overlap**: This refers to the number of characters that overlap between consecutive chunks. Overlapping helps to prevent splitting a single context into multiple pieces, although it does introduce some redundancy across chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#read text from sample-text.txt and print it to the console\n",
    "FILE_NAME=\"sample-text.txt\"\n",
    "with open(FILE_NAME, \"r\", encoding=\"utf-8\") as file:\n",
    "        text=file.read()\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(chunk_size = 50, chunk_overlap=0, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 50, chunk_overlap=10, separator='', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size = 50, chunk_overlap=0, separator='me', strip_whitespace=False)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recursive Character Text Splitting\n",
    "\n",
    "With Character Text Splitter we split text by simply dividing the document based on a fixed number of characters. While straightforward, this method doesn't account for the document's inherent structure.\n",
    "\n",
    "The Recursive Character Text Splitter addresses this limitation by allowing us to specify a series of separators to intelligently split our documents. This method takes into account various structural elements, resulting in a more context-aware split.\n",
    "\n",
    "#### Default Separators in LangChain\n",
    "\n",
    "Let's examine the default separators used in LangChain:\n",
    "\n",
    "- `\"\\n\\n\"`: Double new line, commonly indicating paragraph breaks.\n",
    "- `\"\\n\"`: Single new line.\n",
    "- `\" \"`: Spaces between words.\n",
    "- `\"\"`: Individual characters.\n",
    "\n",
    "Period (`\".\"`) is not included in the default list of separators. \n",
    "\n",
    "#### Why Choose Recursive Character Text Splitter?\n",
    "\n",
    "The Recursive Character Text Splitter is a versatile tool, often my go-to when prototyping a quick application. Its flexibility in handling various separators makes it an excellent first choice if you're unsure which splitter to use.\n",
    "\n",
    "By understanding and leveraging the structure of your document, this splitter can produce more meaningful and contextually appropriate splits, enhancing the overall processing and analysis of your text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 50, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After splitting the text into paragraphs, the process evaluates the size of each chunk. If a chunk is too large, it will attempt to divide it using the next available separator. Should the chunk remain too large, the process will continue to the subsequent separator, repeating this until an appropriate size is achieved.\n",
    "\n",
    "Given the length of this text, we should consider splitting it using larger separators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size = 500, chunk_overlap=0)\n",
    "text_splitter.create_documents([text])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Document-Specific Splitting\n",
    "\n",
    "It's important to address document types that go beyond simple text files. What if your documents contain images, PDFs, or code snippets? Our initial two levels of chunking strategies may not be effective for these diverse formats, so we need to adopt a different approach.\n",
    "\n",
    "This level focuses on tailoring your chunking strategy to fit various data formats. Let's explore several examples to illustrate this in practice:\n",
    "\n",
    "#### Markdown, Python, and JavaScript Splitters\n",
    "\n",
    "For Markdown, Python, and JavaScript files, the splitters will resemble the Recursive Character method but will use different separators tailored to each format.\n",
    "\n",
    "##### Markdown Splitter\n",
    "Markdown files often contain headings, lists, code blocks, and links. A specialized splitter can use these elements as natural breakpoints.\n",
    "\n",
    "- **Headings**: Split at `#`, `##`, `###`, etc.\n",
    "- **Lists**: Split at `-`, `*`, `1.`, etc.\n",
    "- **Code Blocks**: Split at triple backticks ``` ````\n",
    "- **Links**: Split at `[text](url)`\n",
    "\n",
    "##### Python Splitter\n",
    "Python files have distinct structural elements such as function definitions, class definitions, and comments.\n",
    "\n",
    "- **Function Definitions**: Split at `def`\n",
    "- **Class Definitions**: Split at `class`\n",
    "- **Comments**: Split at `#`\n",
    "\n",
    "##### JavaScript Splitter\n",
    "JavaScript files also have unique structural features like function declarations, import statements, and comments.\n",
    "\n",
    "- **Function Declarations**: Split at `function`\n",
    "- **Import Statements**: Split at `import`\n",
    "- **Comments**: Split at `//` for single-line comments and `/*...*/` for multi-line comments\n",
    "\n",
    "#### Handling Other Formats\n",
    "\n",
    "##### Images\n",
    "Images can be split based on metadata or by grouping related images together. However, splitting images isn't usually necessary unless you're dealing with image datasets.\n",
    "\n",
    "##### PDFs\n",
    "PDFs are complex documents that can contain text, images, and vector graphics. A PDF splitter can use the following strategies:\n",
    "\n",
    "- **Pages**: Split by individual pages\n",
    "- **Headings**: Use text headings to define sections\n",
    "- **Paragraphs**: Split by paragraphs for more granularity\n",
    "\n",
    "##### Code Snippets\n",
    "Code snippets can be language-specific, so the splitting strategy should account for the syntax and structure of the particular language.\n",
    "\n",
    "- **Blocks**: Split at logical code blocks or functions\n",
    "- **Comments**: Use comments as natural breakpoints\n",
    "- **Imports/Includes**: Split at import or include statements\n",
    "\n",
    "By customizing your chunking strategy to fit the specific format of your documents, you can better manage and process diverse types of data. This tailored approach ensures that you handle different data formats effectively, maintaining the integrity and meaning of the original content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MD_FILE_NAME=\"sample-markdown.md\"\n",
    "with open(MD_FILE_NAME, \"r\", encoding=\"utf-8\") as file:\n",
    "        markdown_txt=file.read()\n",
    "print(markdown_txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import MarkdownTextSplitter\n",
    "splitter = MarkdownTextSplitter(chunk_size = 100, chunk_overlap=0)\n",
    "splitter.create_documents([markdown_txt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Semantic Chunking\n",
    "\n",
    "Semantic Chunking involves dividing text based on semantic similarity. This technique helps in creating more coherent and contextually relevant text segments.\n",
    "\n",
    "#### Overview\n",
    "The process begins by splitting the text into individual sentences. These sentences are then grouped into clusters of three. Subsequently, similar clusters are merged based on their proximity in the embedding space, ensuring that the final chunks are semantically cohesive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --quiet langchain_experimental langchain_openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_experimental.text_splitter import SemanticChunker\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "#from langchain_openai.embeddings import OpenAIEmbeddings\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "FILE_NAME=\"samp1.txt\"\n",
    "with open(FILE_NAME, \"r\", encoding=\"utf-8\") as file:\n",
    "        text=file.read()\n",
    "\n",
    "azure_openai_endpoint = os.environ[\"AZURE_OPENAI_ENDPOINT\"]\n",
    "azure_openai_key = os.environ[\"AZURE_OPENAI_API_KEY\"] if len(os.environ[\"AZURE_OPENAI_API_KEY\"]) > 0 else None\n",
    "azure_openai_embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT_NAME\"]\n",
    "embedding_model_name = os.environ[\"AZURE_OPENAI_EMBEDDING_MODEL\"]\n",
    "azure_openai_api_version = os.environ[\"OPENAI_API_VERSION\"]\n",
    "openai_api_type = os.environ[\"OPENAI_API_TYPE\"]\n",
    "\n",
    "\n",
    "\n",
    "embeddings = AzureOpenAIEmbeddings(\n",
    "    model=embedding_model_name,\n",
    "    deployment=embedding_model_name,\n",
    "    openai_api_type = openai_api_type,\n",
    "    openai_api_version = azure_openai_api_version,\n",
    "    azure_endpoint = azure_openai_endpoint,\n",
    "    openai_api_key = azure_openai_key,\n",
    "    embedding_ctx_length=8191,\n",
    "    chunk_size=1000,\n",
    "    max_retries=6\n",
    ")\n",
    "\n",
    "text_splitter = SemanticChunker(AzureOpenAIEmbeddings())\n",
    "docs = text_splitter.create_documents([text])\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(docs[1].page_content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
